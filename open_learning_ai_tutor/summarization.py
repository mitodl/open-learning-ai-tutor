import json
import logging
from typing import Any, cast
from uuid import uuid4

from langchain_core.language_models import LanguageModelLike
from langchain_core.messages import (
    AIMessage,
    AnyMessage,
    HumanMessage,
    RemoveMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.messages.utils import count_tokens_approximately
from langchain_core.prompt_values import ChatPromptValue
from langchain_core.prompts import ChatPromptTemplate
from langgraph.utils.runnable import RunnableCallable
from langmem.short_term import RunningSummary
from langmem.short_term.summarization import (
    DEFAULT_EXISTING_SUMMARY_PROMPT,
    DEFAULT_FINAL_SUMMARY_PROMPT,
    DEFAULT_INITIAL_SUMMARY_PROMPT,
    SummarizationResult,
    TokenCounter,
)
from pydantic import BaseModel
from langchain_core.messages.utils import count_tokens_approximately
from open_learning_ai_tutor.prompts import INITIAL_SUMMARY_PROMPT, EXISTING_SUMMARY_PROMPT, FINAL_SUMMARY_PROMPT    
from langgraph.graph import END, START, MessagesState, StateGraph

log = logging.getLogger(__name__)

def summarize_messages(  # noqa: PLR0912, PLR0913, PLR0915, C901
    messages: list[AnyMessage],
    *,
    running_summary: RunningSummary | None,
    model: LanguageModelLike,
    max_tokens: int,
    max_tokens_before_summary: int | None = None,
    max_summary_tokens: int = 256,
    token_counter: TokenCounter = count_tokens_approximately,
    initial_summary_prompt: ChatPromptTemplate = DEFAULT_INITIAL_SUMMARY_PROMPT,
    existing_summary_prompt: ChatPromptTemplate = DEFAULT_EXISTING_SUMMARY_PROMPT,
    final_prompt: ChatPromptTemplate = DEFAULT_FINAL_SUMMARY_PROMPT,
) -> SummarizationResult:
    """
    Summarize and replace messages when they exceed a token limit.

    This function is based on the `langmem.short_term.summarization.summarize_messages`
    function with only minor changes (it omits the strict removal of earlier messages to
    summarize depending on the max token count).

    This function processes the messages from oldest to newest: once the cumulative
    number of message  tokens reaches `max_tokens_before_summary`, all messages within
    `max_tokens_before_summary` are summarized (excluding the system message, if any)
    and replaced with a new summary message. The resulting list of messages is
    [summary_message] + remaining_messages.

    Args:
        messages: The list of messages to process.
        running_summary: Optional running summary w/info about previous summarization.
        If provided:
            - only messages that were **not** previously summarized will be processed
            - if no new summary is generated, the running summary will be added to the
                returned messages
            - if a new summary needs to be generated, it is generated by incorporating
                the existing summary value from the running summary
        model: The language model to use for generating summaries.
        max_tokens: Maximum number of tokens to return in the final output. Will be
            enforced only after summarization.
        max_tokens_before_summary: Maximum number of tokens to accumulate before
            triggering summarization. Defaults to the same value as `max_tokens` if not
            provided. This allows fitting more tokens into the summarization LLM,
            if needed.
        max_summary_tokens: Maximum number of tokens to budget for the summary.
        token_counter: Function to count tokens in a message. Defaults to approximate
            counting.For more accurate counts use `model.get_num_tokens_from_messages`.
        initial_summary_prompt: Prompt template for generating the first summary.
        existing_summary_prompt: Prompt template for updating a running summary.
        final_prompt: Prompt template w/summary & remaining messages before returning.

    Returns:
        SummarizationResult object containing the updated messages and running summary.
            - messages: list of updated messages ready to be input to the LLM
            - running_summary: RunningSummary object
                - summary: text of the latest summary
                - summarized_message_ids: set of previously summarized message IDs
                - last_summarized_message_id: ID of the last message that was summarized
    """
    # Set max_tokens_before_summary to max_tokens if not provided
    if max_tokens_before_summary is None:
        max_tokens_before_summary = max_tokens

    max_tokens_to_summarize = max_tokens
    # Adjust the remaining token budget to account for the summary to be added
    max_remaining_tokens = max_tokens - max_summary_tokens
    # First handle system message if present

    if messages and isinstance(messages[0], SystemMessage):
        existing_system_message = messages[0]
        # remove the system message from the list of messages to summarize
        messages = messages[1:]
        # adjust remaining token budget for the system msg to be re-added
        max_remaining_tokens -= token_counter([existing_system_message])
    else:
        existing_system_message = None

    # Summarize only when last message is a human message
    if not messages:
        return SummarizationResult(
            running_summary=running_summary,
            messages=(
                messages
                if existing_system_message is None
                else [existing_system_message, *messages]
            ),
        )

    print("I AM SUMMARIZING MESSAGES")
    # Get previously summarized messages, if any
    summarized_message_ids = set()
    total_summarized_messages = 0
    existing_summary = running_summary
    if running_summary:
        summarized_message_ids = running_summary.summarized_message_ids
        # Adjust the summarization token budget to account for the previous summary
        max_tokens_to_summarize -= token_counter(
            [SystemMessage(content=running_summary.summary, id=uuid4())]
        )
        # If we have an existing running summary, find how many messages have been
        # summarized so far based on the last summarized message ID.
        for i, message in enumerate(messages):
            if message.id == running_summary.last_summarized_message_id:
                total_summarized_messages = i + 1
                break

    # We will use this to ensure that the total number of resulting tokens
    # will fit into max_tokens window.
    total_n_tokens = token_counter(messages[total_summarized_messages:])

    # Go through messages to count tokens and find cutoff point
    n_tokens = 0
    idx = max(0, total_summarized_messages - 1)
    # map tool call IDs to their corresponding tool messages
    tool_call_id_to_tool_message: dict[str, ToolMessage] = {}
    should_summarize = False
    for i in range(total_summarized_messages, len(messages)):
        message = messages[i]
        if message.id is None:
            err = f"Messages are required to have ID field: {message}"
            raise ValueError(err)

        if message.id in summarized_message_ids:
            err = f"Message with ID {message.id} has already been summarized."
            raise ValueError(err)

        # Store tool messages by their tool_call_id for later reference
        if isinstance(message, ToolMessage) and message.tool_call_id:
            tool_call_id_to_tool_message[message.tool_call_id] = message

        n_tokens += token_counter([message])

        # Check if we've reached max_tokens_to_summarize and
        # final message is a valid type to end summarization on
        # (not a tool message or AI tool)
        print(message.content)
        print("n_tokens:", n_tokens)
        print("max_tokens_before_summary:", max_tokens_before_summary)
        print("total_n_tokens", total_n_tokens)
        print("max_remaining_tokens:", max_remaining_tokens)
        if (
            n_tokens >= max_tokens_before_summary
            and total_n_tokens - n_tokens <= max_remaining_tokens
            and not should_summarize
            and not isinstance(message, ToolMessage)
            and (not isinstance(message, AIMessage) or not message.tool_calls)
        ):
            should_summarize = True
            idx = i

    print("should_summarize:", should_summarize)
    if not should_summarize:
        messages_to_summarize = []
    else:
        messages_to_summarize = messages[total_summarized_messages : idx + 1]

    if messages_to_summarize:
        if running_summary:
            summary_messages = cast(
                ChatPromptValue,
                existing_summary_prompt.invoke(
                    {
                        "messages": messages_to_summarize,
                        "existing_summary": running_summary.summary,
                    }
                ),
            )
        else:
            summary_messages = cast(
                ChatPromptValue,
                initial_summary_prompt.invoke({"messages": messages_to_summarize}),
            )
        log.debug("messages to summarize: %s", messages_to_summarize)
        summary_response = model.invoke(summary_messages.messages)
        log.debug("Summarization response: %s", summary_response.content)
        summarized_message_ids = summarized_message_ids | {
            message.id for message in messages_to_summarize
        }
        total_summarized_messages += len(messages_to_summarize)
        running_summary = RunningSummary(
            summary=summary_response.content,
            summarized_message_ids=summarized_message_ids,
            last_summarized_message_id=messages_to_summarize[-1].id,
        )

    if running_summary:
        # Only include system message if it doesn't overlap with the existing summary.
        # This is useful if the messages passed to summarize_messages already include a
        # system message with summary. This usually happens when summarization node
        # overwrites the message history.
        include_system_message = existing_system_message and not (
            existing_summary
            and existing_summary.summary in existing_system_message.content
        )

        updated_messages = cast(
            ChatPromptValue,
            final_prompt.invoke(
                {
                    "system_message": [existing_system_message]
                    if include_system_message
                    else [],
                    "summary": running_summary.summary,
                    "messages": messages[total_summarized_messages:],
                }
            ),
        )
        return SummarizationResult(
            running_summary=running_summary,
            messages=updated_messages.messages,
        )
    else:
        # no changes are needed
        return SummarizationResult(
            running_summary=None,
            messages=(
                messages
                if existing_system_message is None
                else [existing_system_message, *messages]
            ),
        )


class CustomSummarizationNode(RunnableCallable):
    """
    Customized implementation of langmem.short_term.SummarizationNode.  The original
    has a bug causing the most recent user question and answer to be lost when the
    summary was updated.
    """

    def __init__(  # noqa: PLR0913
        self,
        *,
        model: LanguageModelLike,
        max_tokens: int,
        max_tokens_before_summary: int | None = None,
        max_summary_tokens: int = 256,
        token_counter: TokenCounter = count_tokens_approximately,
        initial_summary_prompt: ChatPromptTemplate = DEFAULT_INITIAL_SUMMARY_PROMPT,
        existing_summary_prompt: ChatPromptTemplate = DEFAULT_EXISTING_SUMMARY_PROMPT,
        final_prompt: ChatPromptTemplate = DEFAULT_FINAL_SUMMARY_PROMPT,
        input_messages_key: str = "messages",
        output_messages_key: str = "summarized_messages",
        name: str = "summarization",
    ) -> None:
        """
        Initialize the CustomSummarizationNode.
        """
        super().__init__(self._func, name=name, trace=False)
        self.model = model
        self.max_tokens = max_tokens
        self.max_tokens_before_summary = max_tokens_before_summary
        self.max_summary_tokens = max_summary_tokens
        self.token_counter = token_counter
        self.initial_summary_prompt = initial_summary_prompt
        self.existing_summary_prompt = existing_summary_prompt
        self.final_prompt = final_prompt
        self.input_messages_key = input_messages_key
        self.output_messages_key = output_messages_key

    def _func(self, node_input: dict[str, Any] | BaseModel) -> dict[str, Any]:
        """
        Generate a summary if needed.
        Incorporate the previous summary if present.
        Return the summary, plus most recent user input and AI response as messages
        """
        if isinstance(node_input, dict):
            messages = node_input.get(self.input_messages_key)
            context = node_input.get("context", {})
        elif isinstance(node_input, BaseModel):
            messages = getattr(node_input, self.input_messages_key, None)
            context = getattr(node_input, "context", {})
        else:
            error = f"Invalid input type: {type(node_input)}"
            raise TypeError(error)

        if messages is None:
            error = f"Missing required field `{self.input_messages_key}` in the input."
            raise ValueError(error)

        last_message = messages[-1] if messages else None
        previous_summary = context.get("running_summary")
        log.debug("Previous summary:\n\n%s\n\n", previous_summary or "N/A")
        summarization_result = summarize_messages(
            # If we are returning here from a tool call, don't include the last tool
            # message or the preceding AI message that called it.
            messages[:-2] if isinstance(last_message, ToolMessage) else messages,
            running_summary=previous_summary,
            model=self.model,
            max_tokens=self.max_tokens,
            max_tokens_before_summary=self.max_tokens_before_summary,
            max_summary_tokens=self.max_summary_tokens,
            token_counter=self.token_counter,
            initial_summary_prompt=self.initial_summary_prompt,
            existing_summary_prompt=self.existing_summary_prompt,
            final_prompt=self.final_prompt,
        )


        if (
            summarization_result.messages
            and summarization_result.messages[-1] != last_message
        ):
            # Put back the AI/Tool messages, the agent will need them
            summarization_result.messages.extend(
                messages[-2:] if isinstance(last_message, ToolMessage) else messages[-1]
            )
        state_update = {self.output_messages_key: summarization_result.messages}

        if (
            not previous_summary
            or summarization_result.running_summary.summary != previous_summary.summary
        ):
            # The running summary has changed, update the context to include
            # the latest summarization result
            log.debug("New summary:\n\n%s\n\n", summarization_result.running_summary)
            state_update["context"] = {
                **context,
                "running_summary": summarization_result.running_summary,
            }
            # If the input and output messages keys are the same, we need to remove the
            # summarized messages from the resulting message list
            if self.input_messages_key == self.output_messages_key:
                state_update[self.output_messages_key].extend(
                    [
                        RemoveMessage(id=m.id)
                        for m in messages
                        if m.id
                        in summarization_result.running_summary.summarized_message_ids
                    ],
                )

        # Insert/update the summary into the messages to be sent to the agent
        # if it isn't already there.
        output_messages = state_update[self.output_messages_key]
        if (
            summarization_result.running_summary
            and summarization_result.running_summary.summary
            and output_messages
        ):
            # Check if the summary is already present in the output message
            summary_message = [
                msg
                for msg in state_update[self.output_messages_key]
                if hasattr(msg, 'content') and summarization_result.running_summary.summary in msg.content
            ]
            if not summary_message:
                log.debug("Adding summary")
                state_update[self.output_messages_key].insert(
                    1,
                    SystemMessage(
                        id=uuid4(),
                        content=summarization_result.running_summary.summary or "",
                    ),
                )
        log.debug(
            "\n\nMSG: %s\n\n",
            "\nMSG: ".join(
                str(m.content) for m in state_update[self.output_messages_key] if hasattr(m, 'content')
            ),
        )

        return state_update


class SumarizationAgent:
    def __init__(self, client) -> None:
        max_tokens = 8000
        max_summary_tokens= 8000

        def call_model(state: MessagesState):
            messages = state["messages"]
            response = self.client.invoke(messages)
            # We return a list, because this will get added to the existing list
            return {"messages": [response]}
        
        summarization_node = CustomSummarizationNode(
            token_counter=count_tokens_approximately,
            model=client.bind(max_tokens=max_summary_tokens),
            max_tokens=int(max_tokens),
            max_tokens_before_summary=max_tokens,
            max_summary_tokens=max_summary_tokens,
            output_messages_key="llm_input_messages",
            initial_summary_prompt=INITIAL_SUMMARY_PROMPT,
            existing_summary_prompt=EXISTING_SUMMARY_PROMPT,
            final_prompt=FINAL_SUMMARY_PROMPT,
        )

        workflow = StateGraph(MessagesState)

        workflow.add_node("summarize", summarization_node)

        workflow.add_edge(START, "sumarization")
  
        app = workflow.compile()
        self.app = app

    def get_response(self, prompt):
        return self.app.invoke({"messages": prompt})


